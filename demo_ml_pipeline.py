#!/usr/bin/env python3
"""
Demonstration script for Phase 3 (Machine Learning) unified pipeline.
Shows the orchestration system capabilities and usage patterns.
"""

from pathlib import Path
from phase_learn.main_learn import MLPipelineOrchestrator
from phase_learn import get_logger, run_learn_phase

def demo_orchestration_system():
    """Demonstrate the pipeline orchestration capabilities."""
    logger = get_logger("demo")
    
    print("ðŸ¤– PHASE 3 (MACHINE LEARNING) - PIPELINE DEMONSTRATION")
    print("=" * 65)
    
    print("\nðŸ“‹ PIPELINE OVERVIEW")
    print("-" * 30)
    print("This unified system orchestrates 5 ML steps:")
    print("  1ï¸âƒ£  Embedding Generation (all-MiniLM-L6-v2)")
    print("  2ï¸âƒ£  Topic Modeling (BERTopic - optional)")
    print("  3ï¸âƒ£  Classification (SVM + Logistic Regression)")
    print("  4ï¸âƒ£  Clustering (K-Means + DBSCAN)")
    print("  5ï¸âƒ£  Comprehensive Evaluation & Visualization")
    
    print("\nâš™ï¸  HARDWARE OPTIMIZATION")
    print("-" * 30)
    print("  ðŸ–¥ï¸  CPU: Batch processing for 4c 8t")
    print("  ðŸŽ® GPU: Lightweight models for 4GB")
    print("  ðŸ§  RAM: Memory streaming for 32GB max")
    print("  ðŸ“¦ Models: all-MiniLM-L6-v2 (90MB) vs BERT-large (1.3GB)")
    
    # Initialize orchestrator
    orchestrator = MLPipelineOrchestrator()
    
    print("\nðŸ” DATASET VALIDATION")
    print("-" * 30)
    dataset_valid = orchestrator.validate_dataset()
    
    if dataset_valid:
        print("âœ… Dataset validation: PASSED")
        papers_count = 296  # From previous validation
        print(f"   ðŸ“„ Papers available: {papers_count}")
        print(f"   ðŸ§© Chunks generated: 1,842")
    else:
        print("âŒ Dataset validation: FAILED")
        print("   Cannot proceed without valid dataset")
        return
    
    print("\nðŸ“Š EXPECTED OUTPUTS")
    print("-" * 30)
    expected_outputs = [
        ("ðŸŽ¯", "data/models/classifier/classifier_model.pkl", "Trained classification model"),
        ("ðŸ“ˆ", "data/models/clustering/clustering_results.json", "Clustering analysis results"),
        ("ðŸ“‹", "results/comprehensive_evaluation.json", "Complete evaluation metrics"),
        ("ðŸ“", "results/evaluation_report.txt", "Human-readable analysis"),
        ("ðŸ“Š", "results/figures/comprehensive_evaluation.png", "Performance visualizations"),
        ("ðŸ—‚ï¸ ", "reports/ml_pipeline_execution.json", "Detailed execution log")
    ]
    
    for icon, filepath, description in expected_outputs:
        exists = Path(filepath).exists()
        status = "âœ…" if exists else "âšª"
        print(f"   {status} {icon} {filepath}")
        print(f"      {description}")
    
    print("\nðŸš€ EXECUTION OPTIONS")
    print("-" * 30)
    print("Option 1 - Direct execution:")
    print("    python -m phase_learn.main_learn")
    print()
    print("Option 2 - Python API:")
    print("    from phase_learn import run_learn_phase")
    print("    success = run_learn_phase()")
    print()
    print("Option 3 - Advanced orchestration:")
    print("    from phase_learn.main_learn import MLPipelineOrchestrator")
    print("    orchestrator = MLPipelineOrchestrator()")
    print("    orchestrator.run_paper_classification()  # Individual steps")
    
    print("\nðŸ“‹ PIPELINE STATUS TRACKING")
    print("-" * 30)
    print("Current status:")
    for step, status in orchestrator.pipeline_status.items():
        icon = "âœ…" if status else "âšª"
        step_name = step.replace('_', ' ').title()
        print(f"   {icon} {step_name}")
    
    print(f"\nðŸŽ¯ SUCCESS CRITERIA")
    print("-" * 30)
    print("âœ… Classification accuracy > 70%")
    print("âœ… Clustering silhouette score > 0.25") 
    print("âœ… All expected files generated")
    print("âœ… Comprehensive evaluation completed")
    print("âœ… Hardware constraints respected")
    
    print(f"\nâš¡ READY FOR EXECUTION")
    print("=" * 65)
    print("The pipeline is configured and ready to run!")
    print("Use any of the execution options above to start.")
    print()
    print("ðŸ’¡ TIP: Monitor progress with:")
    print("    tail -f data/logs/learn.log")

def show_current_state():
    """Show current state of the ML pipeline outputs."""
    print("\nðŸ“ CURRENT OUTPUT STATE")
    print("=" * 40)
    
    # Check what's already been generated
    output_categories = {
        "Classification Models": [
            "data/models/classifier/classifier_model.pkl",
            "data/models/classifier/classification_results.json"
        ],
        "Clustering Results": [
            "data/models/clustering/clustering_results.json", 
            "data/models/clustering/cluster_analyses.json"
        ],
        "Evaluations": [
            "results/comprehensive_evaluation.json",
            "results/evaluation_report.txt"
        ],
        "Visualizations": [
            "results/figures/comprehensive_evaluation.png",
            "results/figures/kmeans_tfidf_clusters.png"
        ],
        "Execution Logs": [
            "reports/ml_pipeline_execution.json",
            "data/logs/learn.log"
        ]
    }
    
    for category, files in output_categories.items():
        print(f"\n{category}:")
        for filepath in files:
            exists = Path(filepath).exists()
            size = ""
            if exists:
                try:
                    size_bytes = Path(filepath).stat().st_size
                    if size_bytes > 1024*1024:
                        size = f" ({size_bytes/(1024*1024):.1f}MB)"
                    elif size_bytes > 1024:
                        size = f" ({size_bytes/1024:.1f}KB)"
                    else:
                        size = f" ({size_bytes}B)"
                except:
                    size = ""
            
            status = "âœ…" if exists else "âšª"
            print(f"  {status} {filepath}{size}")
    
    # Count existing outputs
    all_files = []
    for files in output_categories.values():
        all_files.extend(files)
    
    existing_count = sum(1 for f in all_files if Path(f).exists())
    total_count = len(all_files)
    
    print(f"\nSummary: {existing_count}/{total_count} outputs exist ({existing_count/total_count:.1%})")
    
    if existing_count == total_count:
        print("ðŸŽ‰ All pipeline outputs are present!")
    elif existing_count > 0:
        print("âš¡ Some outputs exist - pipeline may have run previously")
    else:
        print("ðŸš€ No outputs yet - ready for first execution")

if __name__ == "__main__":
    # Run demonstration
    demo_orchestration_system()
    
    # Show current state
    show_current_state()
    
    print(f"\n" + "=" * 65)
    print("Demo completed! The unified ML pipeline is ready for use.")
    print("=" * 65)